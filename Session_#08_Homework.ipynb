{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b5ace6",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "In this homework, we'll build a model for predicting if we have an image of a dog or a cat. For this,\n",
    "we will use the \"Dogs & Cats\" dataset that can be downloaded from [Kaggle](https://www.kaggle.com/c/dogs-vs-cats/data). \n",
    "\n",
    "You need to download the `train.zip` file.\n",
    "\n",
    "If you have troubles downloading from Kaggle, use [this link](https://github.com/alexeygrigorev/large-datasets/releases/download/dogs-cats/train.zip) instead:\n",
    "\n",
    "```bash\n",
    "wget https://github.com/alexeygrigorev/large-datasets/releases/download/dogs-cats/train.zip\n",
    "```\n",
    "\n",
    "In the lectures we saw how to use a pre-trained neural network. In the homework, we'll train a much smaller model from scratch. \n",
    "\n",
    "**Note:** You don't need a computer with a GPU for this homework. A laptop or any personal computer should be sufficient. \n",
    "\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "The dataset contains 12,500 images of cats and 12,500 images of dogs. \n",
    "\n",
    "Now we need to split this data into train and validation\n",
    "\n",
    "* Create a `train` and `validation` folders\n",
    "* In each folder, create `cats` and `dogs` folders\n",
    "* Move the first 10,000 images to the train folder (from 0 to 9999) for boths cats and dogs - and put them in respective folders\n",
    "* Move the remaining 2,500 images to the validation folder (from 10000 to 12499)\n",
    "\n",
    "You can do this manually or with Python (check `os` and `shutil` packages).\n",
    "\n",
    "\n",
    "### Model\n",
    "\n",
    "For this homework we will use Convolutional Neural Network (CNN. Like in the lectures, we'll use Keras.\n",
    "\n",
    "You need to develop the model with following structure:\n",
    "\n",
    "* The shape for input should be `(150, 150, 3)`\n",
    "* Next, create a covolutional layer ([`Conv2D`](https://keras.io/api/layers/convolution_layers/convolution2d/)):\n",
    "    * Use 32 filters\n",
    "    * Kernel size should be `(3, 3)` (that's the size of the filter)\n",
    "    * Use `'relu'` as activation \n",
    "* Reduce the size of the feature map with max pooling ([`MaxPooling2D`](https://keras.io/api/layers/pooling_layers/max_pooling2d/))\n",
    "    * Set the pooling size to `(2, 2)`\n",
    "* Turn the multi-dimensional result into vectors using a [`Flatten`](https://keras.io/api/layers/reshaping_layers/flatten/) layer\n",
    "* Next, add a `Dense` layer with 64 neurons and `'relu'` activation\n",
    "* Finally, create the `Dense` layer with 1 neuron - this will be the output\n",
    "    * The output layer should have an activation - use the appropriate activation for the binary classification case\n",
    "\n",
    "As optimizer use [`SGD`](https://keras.io/api/optimizers/sgd/) with the following parameters:\n",
    "\n",
    "* `SGD(lr=0.002, momentum=0.8)`\n",
    "\n",
    "\n",
    "For clarification about kernel size and max pooling, check [Week #11 Office Hours](https://www.youtube.com/watch?v=1WRgdBTUaAc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d57ec666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, GlobalAveragePooling2D, Dropout, BatchNormalization, InputLayer, Input\n",
    "from tensorflow.keras.applications.xception import Xception, preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "994630e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Layers\n",
    "Layers = [\n",
    "    keras.Input(shape=(150, 150, 3), name='InputLayer'),\n",
    "    keras.layers.Conv2D(filters = 32,\n",
    "                        kernel_size = (3, 3),\n",
    "                        activation = 'relu',\n",
    "                        name = 'ConvolutionalLayer'\n",
    "                        ),\n",
    "    keras.layers.MaxPool2D(pool_size = (2, 2), name = 'MaxPooling'),\n",
    "    keras.layers.Flatten(name='Flatten'),\n",
    "    keras.layers.Dense(units = 64, activation = 'relu', name = 'Inner'),\n",
    "    keras.layers.Dense(units=1, activation='sigmoid', name='Outer')\n",
    "]\n",
    "\n",
    "# Defining Model\n",
    "model = Sequential(Layers)\n",
    "\n",
    "# Defining Optimizer\n",
    "optimizer = keras.optimizers.SGD(learning_rate = 0.002, momentum = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca26f13",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Since we have a binary classification problem, what is the best loss function for us?\n",
    "\n",
    "Note: since we specify an activation for the output layer, we don't need to set `from_logits=True`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49d98e0",
   "metadata": {},
   "source": [
    "**Binary Cross Entropy** is the best loss fuction to be used for binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd0d5d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deining Loss Function\n",
    "loss = keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# Model compilation\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "              optimizer = optimizer,\n",
    "              metrics=['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083f9948",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "What's the total number of parameters of the model? You can use the `summary` method for that. \n",
    "\n",
    "\n",
    "### Generators and Training\n",
    "\n",
    "For the next two questions, use the following data generator for both train and validation:\n",
    "\n",
    "```python\n",
    "ImageDataGenerator(rescale=1./255)\n",
    "```\n",
    "\n",
    "* We don't need to do any additional pre-processing for the images.\n",
    "* When reading the data from train/val directories, check the `class_mode` parameter. Which value should it be for a binary classification problem?\n",
    "* Use `batch_size=20`\n",
    "* Use `shuffle=True` for both training and validaition \n",
    "​\n",
    "For training use `.fit()` with the following params:\n",
    "​\n",
    "```python\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50\n",
    ")\n",
    "```\n",
    "​\n",
    "Note `validation_steps=50` - this parameter says \"run only 50 steps on the validation data for evaluating the results\". \n",
    "This way we iterate a bit faster, but don't use the entire validation dataset.\n",
    "That's why it's important to shuffle the validation dataset as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d59a0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "ConvolutionalLayer (Conv2D)  (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "MaxPooling (MaxPooling2D)    (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "Flatten (Flatten)            (None, 175232)            0         \n",
      "_________________________________________________________________\n",
      "Inner (Dense)                (None, 64)                11214912  \n",
      "_________________________________________________________________\n",
      "Outer (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 11,215,873\n",
      "Trainable params: 11,215,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "439d3f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "036ed8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n",
      "Found 5000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "                                    directory = \"./Datasets/Session#08/train/\",\n",
    "                                    class_mode = \"binary\",\n",
    "                                    target_size = (150, 150),\n",
    "                                    batch_size = 20,\n",
    "                                    shuffle = True\n",
    "                                )\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "                                        directory = \"./Datasets/Session#08/validation/\",\n",
    "                                        class_mode = \"binary\",\n",
    "                                        target_size = (150, 150),\n",
    "                                        batch_size = 20,\n",
    "                                        shuffle = True\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2437bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 54s 543ms/step - loss: 0.6957 - accuracy: 0.5210 - val_loss: 0.6881 - val_accuracy: 0.5100\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.6874 - accuracy: 0.5500 - val_loss: 0.6813 - val_accuracy: 0.5800\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.6805 - accuracy: 0.5725 - val_loss: 0.6828 - val_accuracy: 0.5680\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 44s 442ms/step - loss: 0.6869 - accuracy: 0.5425 - val_loss: 0.6797 - val_accuracy: 0.5700\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 42s 420ms/step - loss: 0.6801 - accuracy: 0.5640 - val_loss: 0.6816 - val_accuracy: 0.5580\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 40s 402ms/step - loss: 0.6782 - accuracy: 0.5670 - val_loss: 0.6723 - val_accuracy: 0.5870\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 39s 389ms/step - loss: 0.6759 - accuracy: 0.5845 - val_loss: 0.6629 - val_accuracy: 0.6090\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 38s 381ms/step - loss: 0.6648 - accuracy: 0.5965 - val_loss: 0.6746 - val_accuracy: 0.5680\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 37s 366ms/step - loss: 0.6658 - accuracy: 0.6045 - val_loss: 0.6740 - val_accuracy: 0.5810\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 0.6612 - accuracy: 0.5960 - val_loss: 0.6586 - val_accuracy: 0.5990\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "                train_generator,\n",
    "                steps_per_epoch=100,\n",
    "                epochs=10,\n",
    "                validation_data=validation_generator,\n",
    "                validation_steps=50\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56acb18f",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "What is the median of training accuracy for this model?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3640733c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5697499811649323"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "# Median of Training Accuracy\n",
    "hist.accuracy.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf226fad",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "What is the standard deviation of training loss for this model?\n",
    "\n",
    "### Data Augmentation\n",
    "\n",
    "For the next two questions, we'll generate more data using data augmentations. \n",
    "\n",
    "Add the following augmentations to your training data generator:\n",
    "\n",
    "* `rotation_range=40,`\n",
    "* `width_shift_range=0.2,`\n",
    "* `height_shift_range=0.2,`\n",
    "* `shear_range=0.2,`\n",
    "* `zoom_range=0.2,`\n",
    "* `horizontal_flip=True,`\n",
    "* `fill_mode='nearest'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c37da0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011064168604158556"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard Deviation of Training Loss \n",
    "hist.loss.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "336ca5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n",
      "Found 5000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_augen= ImageDataGenerator(rescale=1./255,\n",
    "                                rotation_range = 40,\n",
    "                                width_shift_range=0.2,\n",
    "                                height_shift_range=0.2,\n",
    "                                shear_range=0.2,\n",
    "                                zoom_range=0.2,\n",
    "                                horizontal_flip=True,\n",
    "                                fill_mode='nearest'\n",
    "                                )\n",
    "\n",
    "train_ds = train_augen.flow_from_directory(\n",
    "                                directory = \"./Datasets/Session#08/train/\",\n",
    "                                class_mode = \"binary\",\n",
    "                                target_size = (150, 150),\n",
    "                                batch_size = 20,\n",
    "                                shuffle = True\n",
    "                            )\n",
    "\n",
    "val_augen = ImageDataGenerator()\n",
    "\n",
    "val_ds = val_augen.flow_from_directory(\n",
    "                            directory = \"./Datasets/Session#08/validation/\",\n",
    "                            class_mode = \"binary\",\n",
    "                            target_size = (150, 150),\n",
    "                            batch_size = 20,\n",
    "                            shuffle = True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6dfd15",
   "metadata": {},
   "source": [
    "### Question 5 \n",
    "\n",
    "Let's train our model for 10 more epochs using the same code as previously.\n",
    "Make sure you don't re-create the model - we want to continue training the model\n",
    "we already started training.\n",
    "\n",
    "What is the mean of validation loss for the model trained with augmentations?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92d9ac20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 66s 657ms/step - loss: 0.6794 - accuracy: 0.5685 - val_loss: 0.6526 - val_accuracy: 0.6340\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 58s 575ms/step - loss: 0.6748 - accuracy: 0.5805 - val_loss: 0.6673 - val_accuracy: 0.6030\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 51s 515ms/step - loss: 0.6673 - accuracy: 0.5875 - val_loss: 0.6443 - val_accuracy: 0.6470\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 49s 491ms/step - loss: 0.6770 - accuracy: 0.5635 - val_loss: 0.6541 - val_accuracy: 0.6200\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.6733 - accuracy: 0.5855 - val_loss: 0.6583 - val_accuracy: 0.6030\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.6662 - accuracy: 0.5890 - val_loss: 0.6468 - val_accuracy: 0.6410\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 56s 561ms/step - loss: 0.6686 - accuracy: 0.5820 - val_loss: 0.6496 - val_accuracy: 0.6320\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 60s 599ms/step - loss: 0.6645 - accuracy: 0.5905 - val_loss: 0.6308 - val_accuracy: 0.6390\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 58s 576ms/step - loss: 0.6684 - accuracy: 0.5910 - val_loss: 0.6488 - val_accuracy: 0.6010\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 44s 437ms/step - loss: 0.6638 - accuracy: 0.5960 - val_loss: 0.6374 - val_accuracy: 0.6280\n"
     ]
    }
   ],
   "source": [
    "hist_ = model.fit(\n",
    "                train_ds,\n",
    "                steps_per_epoch=100,\n",
    "                epochs=10,\n",
    "                validation_data=validation_generator,\n",
    "                validation_steps=50\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e64fa0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6490042865276336"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist1 = pd.DataFrame(hist_.history)\n",
    "#  Mean of Validation Loss for the model trained with augmentations\n",
    "hist1.val_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510f463a",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "What's the average of validation accuracy for the last 5 epochs (from 6 to 10)\n",
    "for the model trained with augmentations?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3a13675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6282000064849853"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Average of Validation Accuracy for the last 5 epochs (from 6 to 10) for the model trained with augmentations\n",
    "hist1.val_accuracy[-5:].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
